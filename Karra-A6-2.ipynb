{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A6.2 Tic Tac Toe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment you will run a number of comparisons between different neural networks trained through Q-learning to predict Q functions for Player X and for Player O in a simple Tic Tac Toe game.  \n",
    "\n",
    "All but one simple function is provided, so your effort will be in choosing the parameters for the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neuralnetworks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9b/nsfxppj52z5gfzm9402l24w80000gn/T/ipykernel_15756/4129738535.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mneuralnetworks\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'neuralnetworks'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import neuralnetworks as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_state():\n",
    "    return np.array([0] * 9)\n",
    "\n",
    "def next_state(s, a, marker):  # s is a board, and a is an index into the cells of the board, marker is 1 or -1\n",
    "    s = s.copy()\n",
    "    s[a] = 1 if marker == 'X' else -1\n",
    "    return s\n",
    "\n",
    "def reinforcement(s):\n",
    "    if won('X', s):\n",
    "        return 1\n",
    "    if won('O', s):\n",
    "        return -1\n",
    "    return 0\n",
    "\n",
    "def won(player, s):\n",
    "    marker = 1 if player == 'X' else -1\n",
    "    combos = np.array((0,1,2, 3,4,5, 6,7,8, 0,3,6, 1,4,7, 2,5,8, 0,4,8, 2,4,6))\n",
    "    return np.any(np.all(marker == s[combos].reshape((-1, 3)), axis=1))\n",
    "\n",
    "def draw(s):\n",
    "    return sum(s == 0) == 0\n",
    "\n",
    "def valid_actions(state):\n",
    "    return np.where(state == 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_sa(s, a):\n",
    "    return np.hstack((s, a)).reshape(1, -1)\n",
    "\n",
    "def other_player(player):\n",
    "    return 'X' if player == 'O' else 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(Qnet, state, epsilon):\n",
    "    \n",
    "    actions = valid_actions(state)\n",
    "    \n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Random Move\n",
    "        action = np.random.choice(actions)\n",
    "        \n",
    "    else:\n",
    "        # Greedy Move\n",
    "        np.random.shuffle(actions)\n",
    "        Qs = np.array([Qnet.use(stack_sa(state, a)) for a in actions])\n",
    "        action = actions[np.argmax(Qs)]\n",
    "        \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_samples(Qnets, initial_state_f, next_state_f, reinforcement_f, epsilon):\n",
    "    '''Run one game'''\n",
    "    Samples = {'X': {'SA': [], 'R': [], 'Qn': []},\n",
    "               'O': {'SA': [], 'R': [], 'Qn': []}}\n",
    "\n",
    "    s = initial_state_f()\n",
    "    player = 'X'\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        a = epsilon_greedy(Qnets[player], s, epsilon)\n",
    "        sn = next_state_f(s, a, player)\n",
    "        r = reinforcement_f(sn)\n",
    "\n",
    "        Samples[player]['SA'].append(stack_sa(s, a))\n",
    "        Samples[player]['R'].append(r)   # r is with respect to X\n",
    "        Samples[player]['Qn'].append(0.0)  # fill in layer\n",
    "\n",
    "        if r != 0 or draw(sn):\n",
    "            Samples[other_player(player)]['R'][-1] = r  \n",
    "            break\n",
    "\n",
    "        s = sn\n",
    "        player = other_player(player)  # switch\n",
    "\n",
    "    for player in ['X', 'O']:\n",
    "        Samps = Samples[player]\n",
    "        Samps['SA'] = np.vstack(Samps['SA'])\n",
    "        Samps['R'] = np.array(Samps['R']).reshape(-1, 1)\n",
    "        Samps['Qn'] =  np.array(Samps['Qn']).reshape(-1 ,1)  # this statement added in A6.1\n",
    "\n",
    "    # Assign all Qn's, based on following state, but go every other state to do all X values,\n",
    "    ends_with_O = len(Samples['X']) > len(Samples['O'])\n",
    "    if ends_with_O:\n",
    "        # O wins\n",
    "        Samples['X']['Qn'][:-1] = Qnets['X'].use(Samples['X']['SA'][1:, :])\n",
    "        Samples['O']['Qn'][:-1] = Qnets['O'].use(Samples['O']['SA'][1:])\n",
    "    else:\n",
    "        # X wins or draw\n",
    "        Samples['X']['Qn'][:-1] = Qnets['X'].use(Samples['X']['SA'][1:])\n",
    "        Samples['O']['Qn'][:-1] = Qnets['O'].use(Samples['O']['SA'][1:])\n",
    "\n",
    "    for player in ['X', 'O']:\n",
    "        Samps = Samples[player]\n",
    "        Samps['Qn'] = np.array(Samps['Qn']).reshape(-1, 1)\n",
    "\n",
    "    return Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_status(outcomes, epsilons, n_trials, trial):\n",
    "    if trial == 0:\n",
    "        return\n",
    "    outcomes = np.array(outcomes)\n",
    "    n_per = 10\n",
    "    n_bins = (trial + 1) // n_per\n",
    "    if n_bins == 0:\n",
    "        return\n",
    "    outcome_rows = outcomes[:n_per * n_bins].reshape((-1, n_per))\n",
    "    outcome_rows = outcome_rows[:trial // n_per + 1, :]\n",
    "    avgs = np.mean(outcome_rows, axis=1)\n",
    "    \n",
    "    plt.subplot(3, 1, 1)\n",
    "    xs = np.linspace(n_per, n_per * n_bins, len(avgs))\n",
    "    plt.plot(xs, avgs)\n",
    "    plt.ylim(-1.1, 1.1)\n",
    "    plt.xlabel('Games')\n",
    "    plt.ylabel('Mean of Outcomes') # \\n(0=draw, 1=X win, -1=O win)')\n",
    "    plt.title(f'Bins of {n_per:d} Games')\n",
    "    \n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(xs, np.sum(outcome_rows == -1, axis=1), 'r-', label='Losses')\n",
    "    plt.plot(xs, np.sum(outcome_rows == 0, axis=1), 'b-', label='Draws')\n",
    "    plt.plot(xs, np.sum(outcome_rows == 1, axis=1), 'g-', label='Wins')\n",
    "    plt.legend(loc='center')\n",
    "    plt.ylabel(f'Number of Games\\nin Bins of {n_per:d}')\n",
    "    \n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(epsilons[:trial])\n",
    "    plt.ylabel('$\\epsilon$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_standardization(Qnet, Xmeans, Xstds, Tmeans, Tstds):\n",
    "    Qnet.Xmeans = np.array(Xmeans)\n",
    "    Qnet.Xstds = np.array(Xstds)\n",
    "    Qnet.Tmeans = np.array(Tmeans)\n",
    "    Qnet.Tstds = np.array(Tstds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "def run(X_hidden_units_list_of_lists, O_hidden_units_list_of_lists, n_epochs_list, learning_rate_list, \n",
    "        repetitions=5, graphics=False):\n",
    "    \n",
    "    if graphics:\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        \n",
    "    n_trials = 8000         # number of repetitions of makeSamples-updateQ loop\n",
    "    \n",
    "    gamma = 1.0        # discount factor\n",
    "    final_epsilon = 0.01 # value of epsilon at end of simulation. Decay rate is calculated\n",
    "    epsilon_decay = np.exp(np.log(final_epsilon) / (n_trials)) # to produce this final value\n",
    "\n",
    "    results = []\n",
    "    for n_epochs in n_epochs_list:\n",
    "        for learning_rate in learning_rate_list:\n",
    "            for X_nh in X_hidden_units_list_of_lists:\n",
    "                for O_nh in O_hidden_units_list_of_lists:\n",
    "                \n",
    "                    last_fifth_outcomes = []\n",
    "\n",
    "                    # RRn multiple experiments for these parameter values and average the results\n",
    "                    for rep in range(repetitions):\n",
    "                        \n",
    "                        print(rep + 1, end=' ')\n",
    "                        # Qnet for Player 'X'\n",
    "                        QnetX = nn.NeuralNetwork(9 + 1, X_nh, 1)\n",
    "                        # Qnet for Player 'O'\n",
    "                        QnetO = nn.NeuralNetwork(9 + 1, O_nh, 1)\n",
    "                        Qnets = {'X': QnetX, 'O': QnetO}\n",
    "\n",
    "                        # Inputs are 9 TTT cells plus 1 action\n",
    "                        setup_standardization(QnetX, [0] * 10, [1] * 10, [0], [1])\n",
    "                        setup_standardization(QnetO, [0] * 10, [1] * 10, [0], [1])\n",
    "\n",
    "                        epsilon = 1         # initial epsilon value\n",
    "                        outcomes = []\n",
    "                        epsilon_trace = []\n",
    "\n",
    "                        # Train for n_trials\n",
    "                        for trial in range(n_trials):\n",
    "\n",
    "                            Samples = make_samples(Qnets, initial_state, next_state, reinforcement, epsilon)\n",
    "\n",
    "                            Samps = Samples['X']\n",
    "                            SA = Samps['SA']\n",
    "                            R = Samps['R']\n",
    "                            Qn = Samps['Qn']\n",
    "                            T = R + gamma * Qn\n",
    "                            Qnets['X'].train(SA, T, n_epochs, learning_rate, method='sgd', verbose=False)\n",
    "\n",
    "                            Samps = Samples['O']\n",
    "                            SA = Samps['SA']\n",
    "                            R = - Samps['R']  # r is with respect to X, so negate it\n",
    "                            Qn = Samps['Qn']\n",
    "                            T = R + gamma * Qn\n",
    "                            Qnets['O'].train(SA, T, n_epochs, learning_rate, method='sgd', verbose=False)\n",
    "\n",
    "                            outcomes.append(Samples['X']['R'][-1])\n",
    "                            epsilon_trace.append(epsilon)\n",
    "\n",
    "                            epsilon *= epsilon_decay\n",
    "                            \n",
    "                            if graphics and (trial + 1 == n_trials or trial % (n_trials / 20) == 0):\n",
    "                                plt.clf()\n",
    "                                plot_status(outcomes, epsilon_trace, n_trials, trial)\n",
    "                                clear_output(wait=True)\n",
    "                                display(fig)\n",
    "\n",
    "                        # For each repetition collect the mean of the outcome for the final fifth games\n",
    "                        last_fifth_outcomes.append(np.mean(outcomes[-n_trials // 5:]))\n",
    "                        print(f'{last_fifth_outcomes[-1]:.1f},', end=' ')\n",
    "                        \n",
    "                    results.append([X_nh, O_nh, n_epochs, learning_rate, np.mean(last_fifth_outcomes)])\n",
    "                    print(results[-1])\n",
    "                    \n",
    "    if graphics:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "    return pandas.DataFrame(results, columns=('X_nh', 'O_nh', 'n_epochs', 'lr', 'last_fifth_outcomes')), Qnets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example run with just one value for each of the four parameters.  Only 1 repetition will be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9b/nsfxppj52z5gfzm9402l24w80000gn/T/ipykernel_15756/441397613.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQnets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/9b/nsfxppj52z5gfzm9402l24w80000gn/T/ipykernel_15756/3464382364.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(X_hidden_units_list_of_lists, O_hidden_units_list_of_lists, n_epochs_list, learning_rate_list, repetitions, graphics)\u001b[0m\n\u001b[1;32m     26\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                         \u001b[0;31m# Qnet for Player 'X'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                         \u001b[0mQnetX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_nh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                         \u001b[0;31m# Qnet for Player 'O'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                         \u001b[0mQnetO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO_nh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result, Qnets = run([[]], [[100, 20]], [40], [0.001], 1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': NeuralNetwork(10, [], 1, 'tanh'),\n",
       " 'O': NeuralNetwork(10, [100, 20], 1, 'tanh')}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(10, [], 1, 'tanh') trained for 40 epochs, final training error 0.0614\n",
      "NeuralNetwork(10, [100, 20], 1, 'tanh') trained for 40 epochs, final training error 0.0147\n"
     ]
    }
   ],
   "source": [
    "print(Qnets['X'])\n",
    "print(Qnets['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the following function to compare different neural network structures for X and for O and also try multiple values for `n_epochs` and `learning_rate`.  Include the results of running your function in this notebook.\n",
    "\n",
    "Try at least three different network structures for each player and three different values for `n_epochs` and three different values for `learning_rate`.  Use at least 5 for the second to last argument so your results are averaged over 5 repetitions. Try to find parameters for which O consistently wins, and ones for which X consistently wins. Include these choices in the lists of parameter values in the following function.\n",
    "\n",
    "Discuss your results.  Do they make sense?\n",
    "\n",
    "Here is an example run, though you must use at least three values for each of the first four arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.9, 2 0.9, 3 0.9, 4 0.9, 5 1.0, [[], [], 40, 0.001, 0.944]\n",
      "1 0.9, 2 -0.7, 3 -0.8, 4 -0.8, 5 -0.8, [[], [100, 20], 40, 0.001, -0.45162500000000005]\n",
      "1 -0.9, 2 0.4, 3 0.1, 4 -0.9, 5 0.8, [[], [20, 20, 20], 40, 0.001, -0.10187500000000001]\n",
      "1 1.0, 2 1.0, 3 1.0, 4 1.0, 5 1.0, [[10, 10], [], 40, 0.001, 0.9647499999999999]\n",
      "1 0.4, 2 0.9, 3 0.7, 4 0.8, 5 0.9, [[10, 10], [100, 20], 40, 0.001, 0.75575]\n",
      "1 0.6, 2 0.5, 3 0.9, 4 0.8, 5 0.4, [[10, 10], [20, 20, 20], 40, 0.001, 0.639625]\n",
      "Took 28.0 minutes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_nh</th>\n",
       "      <th>O_nh</th>\n",
       "      <th>n_epochs</th>\n",
       "      <th>lr</th>\n",
       "      <th>last_fifth_outcomes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>40</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>[100, 20]</td>\n",
       "      <td>40</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.451625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>[20, 20, 20]</td>\n",
       "      <td>40</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.101875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[10, 10]</td>\n",
       "      <td>[]</td>\n",
       "      <td>40</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.964750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[10, 10]</td>\n",
       "      <td>[100, 20]</td>\n",
       "      <td>40</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.755750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[10, 10]</td>\n",
       "      <td>[20, 20, 20]</td>\n",
       "      <td>40</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.639625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       X_nh          O_nh  n_epochs     lr  last_fifth_outcomes\n",
       "0        []            []        40  0.001             0.944000\n",
       "1        []     [100, 20]        40  0.001            -0.451625\n",
       "2        []  [20, 20, 20]        40  0.001            -0.101875\n",
       "3  [10, 10]            []        40  0.001             0.964750\n",
       "4  [10, 10]     [100, 20]        40  0.001             0.755750\n",
       "5  [10, 10]  [20, 20, 20]        40  0.001             0.639625"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myresult():\n",
    "    result, Qnets = run(X_hidden_units_list_of_lists=[[], [10, 10]],\n",
    "                        O_hidden_units_list_of_lists=[[], [100, 20], [20, 20, 20]],\n",
    "                        n_epochs_list=[40],\n",
    "                        learning_rate_list=[0.001], \n",
    "                        repetitions=5, graphics=False)\n",
    "    return result\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "result = myresult()\n",
    "\n",
    "print(f'Took {(time.time() - start_time) / 60.0:.1f} minutes.')\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies for X winning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy a version of `myresult()` into next cell that contains a network architecture with which X can win the game.  If you cannot get X to win the game, put one of the strategies you tried here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your observations here.  If you cannot get X to win the game, discuss reasons why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies for O winning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy a version of `myresult()` into next cell that contains a network architecture with which O can win the game.  If you cannot get O to win the game, put one of the strategies you tried here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your observations here.  If you cannot get O to win the game, discuss reasons why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading and Check-in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There is no grader script for this assignment.*  The entirety of your grade is based on trying the required number of experiments and presenting your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit\n",
    "\n",
    "For 1 point of extra credit do the following steps.\n",
    "\n",
    "1. Call `run` using your best parameter values and for 1 repetition.\n",
    "2. Create four boards for which it is `X`'s turn. Using the returned `Qnets` print a display of the Q values generated by `Qnets['X']` in a 3 x 3 table corresponding to the tic tac toe board, for each of these four boards.\n",
    "3. Create four boards for which it is `O`'s turn. Using the returned `Qnets` print a display of the Q values generated by `Qnets['O']` in a 3 x 3 table corresponding to the tic tac toe board, for these four boards.\n",
    "4. Discuss the values.  Do they make sense?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
